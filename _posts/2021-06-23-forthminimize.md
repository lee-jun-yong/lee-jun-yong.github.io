---
layout: single
title: "캐글스터디 4회차 : 경진 대회 주요 모델 "
---

# Chapter 4. 모델 구축

## 4.1 경진대회의 흐름

![1](https://user-images.githubusercontent.com/84025932/123081114-9233a200-d458-11eb-9419-e378fa2e5622.jpg)

전체적인 경진대회 흐름

#### 1. 특징생성

+ 대회에서 주어지는 학습 데이터와 테스트 데이터를 가지고 목적변수에 맞추어 특징을 생성합니다
+ 특징을 생성하는 방법은 앞선 장에서 학습을 했고 저희는 이 특징 생성에 이용되는 데이터셋에 대해 알아보겠습니다.
+ 학습 데이터와 테스트 데이터로 나뉘며 데이터의 구조 train_X 와 test_X 의 구조는 (행데이터의개수 * 특징의 개수) 행렬 형태와 같습니다. 
+ 학습데이터의 train_y는 목적변수로 값이 채워져있고테스트 데이터의 예측값은저희가 학습과 이외의 과정을 통해 구해야하므로 빈값을가지고 있습니다. 

#### 2. 모델구축

>> 1. 모델 정의
>> 2. 특징의 추가 및 변경(학습데이터와 테스트 데이터의 열을 추가)
>> 3. 하이퍼 파라미터 변경
>> 필요시 모델의 종류 변경 

#### 3.모델학습

+ 학습 데이터를 모델에 적용 시키는 과정을 모델 학습
+ Scikit-learn의 fit메서드 이용( model.fit(train_x,train_y) )

#### 4. 학습데이터 예측 및 평가 

+ 검증방식을 통해 학습데이터로 학습시킨 모델에 검증데이터를 가지고 예측값을구하고 이 값을 대회측에서요구하는 평가지표를 가지고 예측값의좋고 나쁨을 평가 합니다.
+ 평가가 나쁘다면 모델 구축 단계로 돌아가 앞선 방법들을 재시행합니다.


#### 5.테스트데이터 예측 및 추출

+ 테스트데이터를 가지고 훈련데이터에서 학습된 모델을 통해 최종 예측값을생성한다.
+ 자신의 예측값을퍼블릭 리더보드에 예측값을보고 맘에 안들면모델 구축과정으로 돌아가 재구축한다.

  ***※ Public leaderboard보면 leaderboard에 과적합이 되므로 private leaderboard의 순위가 떨어질수있다. ***

#### 검증구조구축

모델을 만들어가면서 그 모델이 좋은지나쁜지 평가해야합니다.

이때 이미 학습에 이용한 데이터로 평가하려 해도 모델은 그 정답을 다알고있는 상황(=정보유출)이므로 미지의 데이터에 대한 평가를 할 수가 없습니다.

모델 구축 과정에서 학습데이터의 일부 데이터를 평가용 데이터 (=검증 데이터)로 분류합니다.

이 과정을 홀드아웃 방법이라 합니다. 

![2](https://user-images.githubusercontent.com/84025932/123082351-e3906100-d459-11eb-824c-a268d12f2530.jpg)
홀드아웃방법

+ 홀드아웃을 쓰면 모델의 학습이나 평가에 사용할수있는 데이터가 줄어드므로 효율적인 데이터 사용을 위해 교차 검증이란 방법을 자주 사용합니다.

##### 교차검증
>> 1. 학습 데이터를 여러 개로 분할(fold  생성)
>> 2. 그 중 하나를 검증데이터, 나머지를 학습 데이터로 삼아 학습 및 평가를 실시하고 검증데이터에서의 점수를 구합니다.
>> 3. 분할한 획수 만큼 검증 데이터를 바꿔가며 2의 내용을 반복하여 점수를 구합니다.
>> 4. 검증 데이터의 평균 점수로 모델의 좋고 나쁨을 평가합니다.


## 4.2 경진대회의 주요 모델



### 그레이디언트 부스팅 결정트리(GBDT)


### 그레이디언트 부스팅 결정트리(GBDT)의 개요 

+ 다수의 결정 트리로 이루어집니다.
+ 학습 진행 순서

+ >> ①목적변수와 예측값으로부터계산되는 목적함수를 개선하고자 결정 트리를 작성해 모델해추가
+ >> ②하이퍼파라미터에서정한 결정 트리의 개수만큼 ①을 반복합니다.

+ 각 결정 트리의 분기 및 잎의 가중치가 정해집니다.

![3](https://user-images.githubusercontent.com/84025932/123083182-c60fc700-d45a-11eb-81af-9d3ef675c88d.jpg)
GBDT 구성도 

#### 그레이디언트 부스팅 결정트리(GBDT) 특징

+ 특징은 수치로 표현
    어떤 특징이 어떤 값보다 큰지 작은지에 따라 결정 분기에서 나뉘기 때문입니다.
+ 결측값을 다룰 수 있음
    결측값일 때 결정 트리의 분기에서 어느 한 쪽으로 나뉠 수 있으므로 결측값을채우지 않고 그대로 사용 가능합니다.
+ 변수 간 상호작용이 반영됨
    분기 반복에 따라 변수 간 상호작용이 반영됩니다.
+ 특정값의범위를 스케일링 할 필요가 없음
    결정 트리에서는 각각의 특징에서 값의 크고 작은 관계만이 문제가 되기 때문입니다.
+ 범주형 변수를 원-핫 인코딩하지 않아도 됨
    수치화해야 하므로 레이블 인코딩은 필요, but 대부분 원-핫 인코딩은 필요 x
+ 희소행렬에 대응함
    scipy-sparse 모듈의 csr_matrix나 csc_matrix 등의 희소 행렬 입력 가능합니다.
    
#### 그레이디언트 부스팅 결정트리(GBDT) 쓰이는 라이브러리

+ xgboost 
+ lightgbm
+ catboost

**사이킷런 ensemble 모듈의 GradientBoostingRegressor 클래스와 GradientBoostingClassifier 클래스 또한 GBDT 기반 모델이지만 모델 성능과 계산 속도 모두 xgboost보다 떨어져 많이 쓰이지는 않음**

### xgboost 사용팁

+booster 매개변수에서 모델을 선택할 수 있으며, GBDT를 사용하려면 디폴트값인 gbtree로 설정
+ 목적함수를 최소화하도록 학습이 진행되며, 기본적으로는 매개변수 objective를 아래와 같이 설정함
+ >> 회귀: reg:squarederror를 설정함으로써 평균제곱오차를최소화하도록 학습
+ >> 이진 분류: binary:logistic를 설정함으로써 로그 손실을 최소화하도록 학습
+ >> 다중 클래스 분류: multi:softprob를 설정함으로써 다중 클래스 로그 손실을 최소화하도록 학습


+ 학습률, 결정 트리의 깊이, 정규화 강도 등을 하이퍼파라미터로지정 가능
+ 학습 데이터와 검증 데이터의 점수 모니터링


### 신경망

#### 신경망의 개요

+ 경진대회에 사용되는 정형데이터는 은닉계층이 2~4층 정도로이루어진 다층퍼셉트론
+ 다층퍼셉트론(MLP) :복잡한 딥러닝 알고리즘의 출발점이며, 분류와 회귀에 사용가능합니다. 
+ 다층퍼셉트론의 구조는 입력계층,은닉계층,출력계층 세가지 레이어라 나뉜다. 

![4](https://user-images.githubusercontent.com/84025932/123084226-dc6a5280-d45b-11eb-98fa-3dc7200156a1.jpg)
다층퍼셉트론 구조

##### 입력계층
>> 입력계층: 특징이 입력으로 주어짐
>> 입력계층의 유닛수는 특징의 수와 같음


##### 은닉계층

>> 은닉계층에서는 앞선 입력계층 혹은 은닉계층의 값을 가충치로 부가한 합을 구하여 결합한뒤에 활성화함수를 적용


>> 활성화 함수로는 렐루(ReLu) 이용


>> 은닉계층의 유닛수는 하이퍼파라미터로 설정

![11](https://user-images.githubusercontent.com/84025932/123087086-1d179b00-d45f-11eb-860d-30cef58f35a1.jpg)

##### 출력계층
>> 1. 출력계층에서는 앞 선 은닉계층의 값을 가중치로 부가한 합을 계산
>> 2. 문제에 맞게 활성화 함수를 적용

![6](https://user-images.githubusercontent.com/84025932/123085097-d9239680-d45c-11eb-9753-8dca0798df33.jpg)


>> --출력계층 개수는 회귀나 이진분류라면 1개,다중클래스 분류의 경우 클래스 수

![12](https://user-images.githubusercontent.com/84025932/123087098-1e48c800-d45f-11eb-9fbf-5b621a7db543.jpg)



#### 신경망의 특징

+ 변수값을 수치로 표현
+	신경망 연산 구조상 결측값을 다룰수없음
+	신경망 구조로 부터 비선형성과 변수간의 상호작용을 반영
+	변숫값을 표준화 등으로 스케일링 필요
     변숫값의 크기가 고르지 못하면 학습이 제대로 진행이 되지 않을수 있습니다.
+	하이퍼파라미터의 조정의 어려움으로 인한 모델 성능의 편차
     하이퍼파라미터에 따라서 모델이 과적합되거나 반대로 전혀 학습이 진행되지 않을수 있습니다.
+ 다중클래스 분류에 강점
    구조상 다중클래스 분류를 자연스럽게 모델링 할수있으므로 GBDT와 비교해 좋은 성능이 나올 수 있습니다.
+ GPU로 고속화
    GPU는 원래 그래픽 전용 보드에 이용해 왔다. 그러나 최근에는 그래픽 처리 뿐만 아니라 범용수치 연산도 이용합니다.

#### 신경망의 주요 라이브러리
+ 케라스
+ 파이토치
+ 체이너
+ 텐서플로

#### 신경망의 구현
케라스 라이브러리를 이용해 신경망을 구현

![8](https://user-images.githubusercontent.com/84025932/123085705-8696aa00-d45d-11eb-98f0-9a67ff000b1e.jpg)
![9](https://user-images.githubusercontent.com/84025932/123085709-88606d80-d45d-11eb-8e3d-1cc8254061a6.jpg)

#### 케라스 사용 팁

##### ① 목적함수

Model.complie 단계에서 매개변수 loss에 목적함수를 설정하고 목적함수가 최소화하도록 학습
![10](https://user-images.githubusercontent.com/84025932/123086236-218f8400-d45e-11eb-8fc6-78db5b9a406d.jpg)

##### ② 하이퍼파라미터
> 은닉계층의 계층 수
>	계층별 유닛수 
> 옵티마이저의 종류
> 옵티마이저의 학습률 
> 드롭아웃의 강도

##### ③ 드롭아웃
> 학습시 드롭아웃 대상 계층에서 일부 유닛을 랜덤으로 사용하지 않고 역전파로 가중치를 갱신

##### ④ 콜백
> 미니배치의 처리별 또는 에폭 별로 지정한 처리를 빠르게 진행
> 조기종료
> 모델의 정기적인 저장
> 학습률 스케일링
> 로그 및 가시화

##### ⑤ 임베딩 계층
> 양의정수를 밀집벡터로 변환하는 계층, 모델의 첫번째 층으로만 설정
> 범주형 변수를 입력으로 할때 사용 
> 레이블 인코딩에서 임베딩 적용 방법이 활용
> 자연어를 다룰때 word2vec과 glove와 같이 학습이 끝난 임베딩을 가중치로 설정

##### ⑥ 배치 정규화
> 각 계층의 출력 편차를 미니배치 단위로 표준화함으로써 적절히 억제하는 방법
> 예측시 입력 평균과 표준편차로 표준화가 이뤄지므로 미니배치의 선택방법에 따라 결과가 바뀌는 일은 없음




### 선형모델


#### 선형모델의 개요


모델 자체만으로는 모델 성능이 그리높지 않고 특장점은 없지만 앙상블에서 하나의 모델이나 스태킹의 최종 계층에 적용하는식의 용도로 주로 쓰입니다. 선형모델은 크게 분류문제와 회귀문제로 나뉘는데 각각 모델의 특징을 보겠습니다.

**분류문제**


로지스틱회귀 모델 :선형 회귀에 시그모이드 함수를 적용함으로써 예측값이 가질수 있는 범위를 (0,1) 제한하고 확률을 예측하는 모델


**회귀 문제** 


선형회귀 모델


라소 회귀 : L1정규화를 실시하는 선형 회귀모델


리지 회귀 : L2정규화를 실시하는 선형 회귀모델

L1정규화
>> 계수의 절댓값에 비례하여 벌칙을 주는것
>> 특성 전체를 사용하지 않으면서 기울기를 0에 가깝게 만드는것


L2정규화 
>> 계수의 제곱에 비례하여 벌칙을 주는것
>> 기울기의 모든값을 0에 가깝게 만드는것

#### 선형모델의 특징
+ 특징값은 수치로 표현
+	결측값을 다룰수 없음
+	낮은 모델 성능
+	특징 생성시  세심한 처리 필요
+	학습의 원활함을 위한 특징의 표준화 필요
+	L1정규화에서 예측에 기여하지 않은 특징의 계수는 0 

#### 선형 모델의 주요 라이브러리

+ 사이킷런의 linear_model 모듈
+ Vowpal wabbit

#### 선형모델의 구현
사이킷런의 linear_model 모듈 중에서 로지스틱회귀(LogisticRegression)
사용하여 선형모델을 구현하였습니다.

![13](https://user-images.githubusercontent.com/84025932/123087693-c8c0eb00-d45f-11eb-860e-c02edc3d5095.jpg)

#### 선형 모델의 사용팁

**목적변수**
모델별로 목적변수의 기능이 다름을 나타내는 표입니다.

![14](https://user-images.githubusercontent.com/84025932/123087816-eee68b00-d45f-11eb-9e9e-a50325e1f8bd.jpg)


### 기타 모델_k-최근접 이웃 알고리즘(KNN)

+ KNN은 행 데이터 간 거리를 그들 특징값의 차이로 정의, 그 거리가 가장 가까운 행 데이터 k개의 목적변수로부터 회귀 및 분류 실시
+	사이킷런 neighbors 모듈의 KNeighborClassifier 클래스와 KNeighborRegressor 클래스 사용
+	유클리드 거리 기준으로 거리를 정의함


> 회귀 : 가장 가까운 행 데이터 k개의 목적변수의 평균을 예측값


> 분류 : 가장 가까운 k개의 행 데이터에서 가장 많은 클래스를 예측값


+	값의 규모가 큰 특징이 지나치게 중요시되지 않게 특징에 표준화 등의 스케일링 필요

### 랜덤 포레스트(RF)

+ 다수의 결정 트리의 조합으로 예측하는 모델
+	GBDT와 달리 병렬로 결정 트리를 작성
+	각 결정 트리의 학습에서 행 데이터나 특징을 샘플링해 전달
+	랜덤 포레스트는 사이킷런 ensemble 모듈의 RandomForestClassifier  클래스와 RandomForestRegressor 클래스 사용

#### 랜덤포레스트 모델 구축순서
①	학습 데이터에서 행 데이터를 샘플링해 추출


②	①을 학습하고 결정 트리 작성. 분기 작성 시 특징의 일부만 샘플링해 추출하고 특징의 후보로 삼음. 그 후보들로부터 데이터를 가장 잘 분할하는 특징과 임곗값을 선택해 분기로 삼음


③	①과 ②의 과정을 결정 트리의 개수만큼 병렬로 수행

#### 랜덤 포레스트의 결정 트리 작성에 관한 point
+	회귀 문제일 때는 제곱오차, 분류 문제일 때는 지니 불순도가 가장 감소하도록 분기 시행
+	결정 트리마다 원래 개수와 같은 수만큼 행 데이터를 복원 추출하는 부트스트랩 샘플링 실시
+	분기마다 특징의 일부를 샘플링한 것을 후보로 삼고 그 중 분기의 특징 선택. 회귀 문제에서는 샘플링 않고 특징을 후보로 삼으며, 분류    문제는 특징 개수의 제곱근 개수만큼 추출해 후보로 삼음
+	결정 트리의 개수와 모델 성능의 관계
  결정 트리를 병렬로 작성하므로, GBDT와 달리 결정 트리의 개수가 지나치게 증가해 모델 성능이 낮아지는 일은 없음
  But, 어느 정도 증가 후 성능이 더 이상 올라가지 않으며, 결정 트리의 개수는   계산 시간과 성능의 트레이드 오프로 결정
+	OOB(Out-Of-Bag)
  부트스트랩 샘플링에서 추출되지 않은 행
  OOB 데이터 사용시 검증 데이터를 준비하지 않아도 일반화 성능 가늠 가능
+	예측 확률 타당성
  분류 작업 시, GBDT에서는 가중치에 기반을 둔 예측 확률의 로그 손실을 최소화하려 함
  But, 랜덤 포레스트에서는 지니불순도를 최소화하려는 각 결정 트리의 예측값의 평균을 구함
  랜덤 포레스트 방법으로는 예측 확률의 타당성 보장 x -> 왜곡 가능성

### 익스트림 랜덤 트리(ERT)
+ 랜덤 포레스트와 거의 같은 방법으로 모델 구축
+	분기 작성 시, 각 특징으로 데이터를 가장 잘 분할할 수 있는 임곗값 이용 대신, 랜덤 설정한 임곗값을 이용함
+	랜덤 포레스트보다는 과적합하기 조금 어려운 모델
+	사이킷런 ensemble 모듈의 ExtraTreesClassifier  클래스와 ExtraTreesRegressor 클래스 사용

### RGF
+	GBDT 처럼 목적함수에 정규화항을 명시적으로 포함하지만, 다른 방법으로 결정 트리를 작성하고 성장 시킴
+	목적함수 값이 감소하도록 아래 과정을 반복해 결정 트리의 집합 형성
+	RGF 라이브러리 이용

### FFM
+ FM을 발전시킨 모델로, 추천 문제와 잘 어울림
+	범주형 변수의 조합을 평가한 값이 목적변수로 주어지는 문제를 가정
+	사용자, 상품, 장르를 원-핫 인코딩으로 나타내면  ‘사용자 수+ 상품 수+ 장르 수＇를 특징의 개수로 삼고 희소 데이터로 나타낼 수 있음
+	캐글의 Display Advertising Challenge 대회와 Outbrain Click Prediction 대회 등에서 입상한 솔루션의 주요 모델
+	Libffm 이라는 라이브러리를 사용, 그밖에 xlearn과 같은 라이브러리도 있음

-------------------------------------------

"데이터가 뛰어노는 AI놀이터, 캐글" 한빛미디어 인용

오류가 있을시 dothe7847@nate.com 연락부탁드립니다.


<script src="https://utteranc.es/client.js"
        repo="lee-jun-yong/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

