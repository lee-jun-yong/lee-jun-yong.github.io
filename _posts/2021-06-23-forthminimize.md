---
layout: single
title: "캐글스터디 4회차 : 경진 대회 주요 모델 "
---

# Chapter 4. 모델 구축

## 4.1 경진대회의 흐름

![1](https://user-images.githubusercontent.com/84025932/123081114-9233a200-d458-11eb-9419-e378fa2e5622.jpg)

전체적인 경진대회 흐름

#### 1. 특징생성

+ 대회에서 주어지는 학습 데이터와 테스트 데이터를 가지고 목적변수에 맞추어 특징을 생성합니다
+ 특징을 생성하는 방법은 앞선 장에서 학습을 했고 저희는 이 특징 생성에 이용되는 데이터셋에 대해 알아보겠습니다.
+ 학습 데이터와 테스트 데이터로 나뉘며 데이터의 구조 train_X 와 test_X 의 구조는 (행데이터의개수 * 특징의 개수) 행렬 형태와 같습니다. 
+ 학습데이터의 train_y는 목적변수로 값이 채워져있고테스트 데이터의 예측값은저희가 학습과 이외의 과정을 통해 구해야하므로 빈값을가지고 있습니다. 

#### 2. 모델구축

>> 1. 모델 정의
>> 2. 특징의 추가 및 변경(학습데이터와 테스트 데이터의 열을 추가)
>> 3. 하이퍼 파라미터 변경
>> 필요시 모델의 종류 변경 

#### 3.모델학습

+ 학습 데이터를 모델에 적용 시키는 과정을 모델 학습
+ Scikit-learn의 fit메서드 이용( model.fit(train_x,train_y) )

#### 4. 학습데이터 예측 및 평가 

+ 검증방식을 통해 학습데이터로 학습시킨 모델에 검증데이터를 가지고 예측값을구하고 이 값을 대회측에서요구하는 평가지표를 가지고 예측값의좋고 나쁨을 평가 합니다.
+ 평가가 나쁘다면 모델 구축 단계로 돌아가 앞선 방법들을 재시행합니다.


#### 5.테스트데이터 예측 및 추출

+ 테스트데이터를 가지고 훈련데이터에서 학습된 모델을 통해 최종 예측값을생성한다.
+ 자신의 예측값을퍼블릭 리더보드에 예측값을보고 맘에 안들면모델 구축과정으로 돌아가 재구축한다.
  ***※ Public leaderboard보면 leaderboard에 과적합이 되므로 private leaderboard의 순위가 떨어질수있다. ***

#### 검증구조구축

모델을 만들어가면서 그 모델이 좋은지나쁜지 평가해야합니다.
이때 이미 학습에 이용한 데이터로 평가하려 해도 모델은 그 정답을 다알고있는 상황(=정보유출)이므로 미지의 데이터에 대한 평가를 할 수가 없습니다.
모델 구축 과정에서 학습데이터의 일부 데이터를 평가용 데이터 (=검증 데이터)로 분류합니다.
이 과정을 홀드아웃 방법이라 합니다. 

![2](https://user-images.githubusercontent.com/84025932/123082351-e3906100-d459-11eb-824c-a268d12f2530.jpg)
홀드아웃방법

+ 홀드아웃을 쓰면 모델의 학습이나 평가에 사용할수있는 데이터가 줄어드므로 효율적인 데이터 사용을 위해 교차 검증이란 방법을 자주 사용합니다.

##### 교차검증
>> 1. 학습 데이터를 여러 개로 분할(fold  생성)
>> 2. 그 중 하나를 검증데이터, 나머지를 학습 데이터로 삼아 학습 및 평가를 실시하고 검증데이터에서의 점수를 구합니다.
>> 3. 분할한 획수 만큼 검증 데이터를 바꿔가며 2의 내용을 반복하여 점수를 구합니다.
>> 4. 검증 데이터의 평균 점수로 모델의 좋고 나쁨을 평가합니다.


## 4.2 경진대회의 주요 모델



### 그레이디언트 부스팅 결정트리(GBDT)


### 그레이디언트 부스팅 결정트리(GBDT)의 개요 

+ 다수의 결정 트리로 이루어집니다.
+ 학습 진행 순서
>> ①목적변수와 예측값으로부터계산되는 목적함수를 개선하고자 결정 트리를 작성해 모델해추가
>> ②하이퍼파라미터에서정한 결정 트리의 개수만큼 ①을 반복합니다.
+ 각 결정 트리의 분기 및 잎의 가중치가 정해집니다.

![3](https://user-images.githubusercontent.com/84025932/123083182-c60fc700-d45a-11eb-81af-9d3ef675c88d.jpg)
GBDT 구성도 

#### 그레이디언트 부스팅 결정트리(GBDT) 특징

+ 특징은 수치로 표현
    어떤 특징이 어떤 값보다 큰지 작은지에 따라 결정 분기에서 나뉘기 때문입니다.
+ 결측값을 다룰 수 있음
    결측값일 때 결정 트리의 분기에서 어느 한 쪽으로 나뉠 수 있으므로 결측값을채우지 않고 그대로 사용 가능합니다.
+ 변수 간 상호작용이 반영됨
    분기 반복에 따라 변수 간 상호작용이 반영됩니다.
+ 특정값의범위를 스케일링 할 필요가 없음
    결정 트리에서는 각각의 특징에서 값의 크고 작은 관계만이 문제가 되기 때문입니다.
+ 범주형 변수를 원-핫 인코딩하지 않아도 됨
    수치화해야 하므로 레이블 인코딩은 필요, but 대부분 원-핫 인코딩은 필요 x
+ 희소행렬에 대응함
    scipy-sparse 모듈의 csr_matrix나 csc_matrix 등의 희소 행렬 입력 가능합니다.
    
#### 그레이디언트 부스팅 결정트리(GBDT) 쓰이는 라이브러리

+ xgboost 
+ lightgbm
+ catboost

**사이킷런 ensemble 모듈의 GradientBoostingRegressor 클래스와 GradientBoostingClassifier 클래스 또한 GBDT 기반 모델이지만 모델 성능과 계산 속도 모두 xgboost보다 떨어져 많이 쓰이지는 않음**

### xgboost 사용팁

+booster 매개변수에서 모델을 선택할 수 있으며, GBDT를 사용하려면 디폴트값인 gbtree로 설정
+ 목적함수를 최소화하도록 학습이 진행되며, 기본적으로는 매개변수 objective를 아래와 같이 설정함
>> 회귀: reg:squarederror를 설정함으로써 평균제곱오차를최소화하도록 학습
>> 이진 분류: binary:logistic를 설정함으로써 로그 손실을 최소화하도록 학습
>> 다중 클래스 분류: multi:softprob를 설정함으로써 다중 클래스 로그 손실을 최소화하도록 학습
+ 학습률, 결정 트리의 깊이, 정규화 강도 등을 하이퍼파라미터로지정 가능
+ 학습 데이터와 검증 데이터의 점수 모니터링


### 신경망

#### 신경망의 개요

+ 경진대회에 사용되는 정형데이터는 은닉계층이 2~4층 정도로이루어진 다층퍼셉트론
+ 다층퍼셉트론(MLP) :복잡한 딥러닝 알고리즘의 출발점이며, 분류와 회귀에 사용가능합니다. 
+ 다층퍼셉트론의 구조는 입력계층,은닉계층,출력계층 세가지 레이어라 나뉜다. 

![4](https://user-images.githubusercontent.com/84025932/123084226-dc6a5280-d45b-11eb-98fa-3dc7200156a1.jpg)
다층퍼셉트론 구조

##### 입력계층
>> 입력계층: 특징이 입력으로 주어짐
>> 입력계층의 유닛수는 특징의 수와 같음


##### 은닉계층
>> 은닉계층에서는 앞선 입력계층 혹은 은닉계층의 값을 가충치로 부가한 합을 구하여 결합한뒤에 활성화함수를 적용
>> 활성화 함수로는 렐루(ReLu) 이용
>> 은닉계층의 유닛수는 하이퍼파라미터로 설정

![5](https://user-images.githubusercontent.com/84025932/123085090-d7f26980-d45c-11eb-8622-035e354f5973.jpg)

##### 출력계층
>> 1. 출력계층에서는 앞 선 은닉계층의 값을 가중치로 부가한 합을 계산
>> 2. 문제에 맞게 활성화 함수를 적용
![6](https://user-images.githubusercontent.com/84025932/123085097-d9239680-d45c-11eb-9753-8dca0798df33.jpg)
>> 3. 출력계층 개수는 회귀나 이진분류라면 1개,다중클래스 분류의 경우 클래스 수
![7](https://user-images.githubusercontent.com/84025932/123085099-d9bc2d00-d45c-11eb-91b2-808c7ad3c85b.jpg)



#### 신경망의 특징

+ 변수값을 수치로 표현
+	신경망 연산 구조상 결측값을 다룰수없음
+	신경망 구조로 부터 비선형성과 변수간의 상호작용을 반영
+	변숫값을 표준화 등으로 스케일링 필요
     변숫값의 크기가 고르지 못하면 학습이 제대로 진행이 되지 않을수 있습니다.
+	하이퍼파라미터의 조정의 어려움으로 인한 모델 성능의 편차
     하이퍼파라미터에 따라서 모델이 과적합되거나 반대로 전혀 학습이 진행되지 않을수 있습니다.
+ 다중클래스 분류에 강점
    구조상 다중클래스 분류를 자연스럽게 모델링 할수있으므로 GBDT와 비교해 좋은 성능이 나올 수 있습니다.
+ GPU로 고속화
    GPU는 원래 그래픽 전용 보드에 이용해 왔다. 그러나 최근에는 그래픽 처리 뿐만 아니라 범용수치 연산도 이용합니다.

#### 신경망의 주요 라이브러리
+ 케라스
+ 파이토치
+ 체이너
+ 텐서플로

#### 신경망의 구현
케라스 라이브러리를 이용해 신경망을 구현

![8](https://user-images.githubusercontent.com/84025932/123085705-8696aa00-d45d-11eb-98f0-9a67ff000b1e.jpg)
![9](https://user-images.githubusercontent.com/84025932/123085709-88606d80-d45d-11eb-8e3d-1cc8254061a6.jpg)

#### 케라스 사용 팁

##### ① 목적함수

Model.complie 단계에서 매개변수 loss에 목적함수를 설정하고 목적함수가 최소화하도록 학습
![10](https://user-images.githubusercontent.com/84025932/123086236-218f8400-d45e-11eb-8fc6-78db5b9a406d.jpg)

##### ② 하이퍼파라미터
>> 은닉계층의 계층 수
>>	계층별 유닛수 
>> 옵티마이저의 종류
>> 옵티마이저의 학습률 
>> 드롭아웃의 강도

##### ③ 드롭아웃
학습시 드롭아웃 대상 계층에서 일부 유닛을 랜덤으로 사용하지 않고 역전파로 가중치를 갱신

##### ④ 콜백
>> 미니배치의 처리별 또는 에폭 별로 지정한 처리를 빠르게 진행
>> 조기종료
>> 모델의 정기적인 저장
>> 학습률 스케일링
>> 로그 및 가시화

##### ⑤ 임베딩 계층
>> 양의정수를 밀집벡터로 변환하는 계층, 모델의 첫번째 층으로만 설정
>> 범주형 변수를 입력으로 할때 사용 
>> 레이블 인코딩에서 임베딩 적용 방법이 활용
>> 자연어를 다룰때 word2vec과 glove와 같이 학습이 끝난 임베딩을 가중치로 설정

##### ⑥ 배치 정규화
>> 각 계층의 출력 편차를 미니배치 단위로 표준화함으로써 적절히 억제하는 방법
>> 예측시 입력 평균과 표준편차로 표준화가 이뤄지므로 미니배치의 선택방법에 따라 결과가 바뀌는 일은 없음

































































































